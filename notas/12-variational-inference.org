#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Inferencia Aproximada~
#+STARTUP: showall
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session variational-inference :exports both :results output org :tangle ../rscripts/12-variational-inference.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Inferencia Aproximada.\\
*Objetivo*: En esta sección del curso estudiaremos métodos aproximados de
 inferencia. En particular nos concentraremos en estudiar inferencia
 variacional. Lo cual implica utilizar una familia de distribuciones dónde
 buscaremos un sustituto para nuestra distribución posterior. ~Stan~ utiliza un
 método de esta colección de aproximaciones.\\
*Lectura recomendada*: Una explicación de la aproximación Laplace la puedes
 encontrar en 2.4.4 de citep:McElreath2020 y 13.3 de citep:Gelman2014a. Es
 natural encontrar exposición de inferencia variacional en textos de ML como
 citep:Murphy2012 o citep:Bishop2006.  Aunque, también se puede consultar en la
 sección 13.7 de citep:Gelman2014a.
#+END_NOTES


#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

#+begin_src R :exports none :results none
  ## Librerias para modelacion bayesiana
  library(cmdstanr)
  library(posterior)
  library(bayesplot)
#+end_src

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#muestreo-y-aproximaciones][Muestreo y aproximaciones]]
  - [[#aproximación-por-curvatura][Aproximación por curvatura]]
  - [[#aproximación-por-optimización][Aproximación por optimización]]
  - [[#la-solución][La solución]]
  - [[#dirección-de-kl][Dirección de KL]]
  - [[#conclusiones][Conclusiones]]
- [[#inferencia-aproximada][Inferencia aproximada]]
- [[#ejemplo-numérico][Ejemplo numérico]]
- [[#solución-de-campo-medio][Solución de campo medio]]
- [[#conclusiones][Conclusiones]]
:END:


* Introducción

En inferencia Bayesiana definimos un modelo conjunto para ~observaciones~ y las
~configuraciones~ del proceso generador de datos. Esto nos permite utilizar el
teorema de Bayes para actualizar nuestro conocimiento sobre los parámetros que no conocemos
por medio de
#+name: eqn:post
\begin{align}
\pi(\theta | y ) \propto \pi(y | \theta )\, \pi(\theta)\,.
\end{align}

El procedimiento de inferencia dentro de este marco es sencillo y prácticamente directo pues se
traduce en ~reportar la distribución posterior~ de las configuraciones en luz de las observaciones que tengamos. 

#+REVEAL: split
A lo largo de este curso hemos establecido que de alguna u otra forma lo que
necesitamos es reportar valores esperados (que se traduce en poder resolver
integrales) utilizando dicho estado de conocimiento actualizado.

En esta sección estudiaremos mecanismos para utilizar aproximaciones al proceso
de inferencia basado en el lado derecho de  [[eqn:post]].

* Muestreo y aproximaciones

Hasta ahora lo que hemos visto son métodos de ~aproximación de integrales~. En
particular utilizando el ~método Monte Carlo~. Hemos discutido que este es un
método de estimación insesgado del cual se pueden esperar algunas propiedades
bondadosas en el largo plazo.

Con los métodos de ~simulación Markoviana~ esperamos poder eliminar los problemas
de complejidad computacional que usualmente se encuentran en aplicaciones. Por ejemplo, la
incapacidad de utilizar generadores de números aleatorios para cualquiera que sea la distribución dada. 

Los métodos Markovianos generan muestras, que esperamos sean, ligeramente
correlacionadas y cuya distribución corresponda a la distribución que nos interesa.

#+REVEAL: split
Resta estudiar qué alternativas tenemos cuando no podemos esperar a que termine
de correr nuestro muestreador.


** Aproximación por curvatura

Estudiaremos alternativas para poder aproximar el problema de inferencia
Bayesiana. La aproximación ya no es sobre la resolución de una integral. Ahora,
vamos a aproximar la distribución misma utilizando ciertas nociones de optimización. 

#+REVEAL: split
En clase hemos discutido que con un conjunto suficientemente grande de datos la
distribución posterior se /parece/ a una Gaussiana. Parece natural poder,
entonces, construir una aproximación con estas características. Es decir,
\begin{align}
\pi(\theta | y) \approx  \mathsf{Normal}\left( \theta \big| \hat \theta, \Sigma_{\hat \theta} \right)\,,
\end{align}
donde
\begin{gather}
\hat \theta = \mathsf{moda}(\theta|y), \qquad \Sigma_{\hat \theta} = \left[ - \nabla^2_\theta \log \pi(\hat \theta|y) \right]^{-1}\,.
\end{gather}
#+REVEAL: split
La aproximación utiliza información de primero y segundo orden de la
distribución posterior. Es decir
\begin{align}
\hat \theta = \arg \max_\theta \pi(\theta| y)\,,
\end{align}
y $\Sigma_\theta$ nos da la información de la curvatura. Esta ~aproximación
cuadrática~ se denomina ~aproximación de Laplace~.

#+BEGIN_NOTES
Para que la aproximación de Laplace tenga sentido para un problema con
parámetros restringidos, es usual transformar los parámetros a una escala sin
restricciones. Por ejemplo, utilizando una transformación logarítmica o /logit/ y
recordando incorporar el término multiplicativo de la Jacobiana de dicha
transformación.
#+END_NOTES

#+REVEAL: split
La aproximación de Laplace nos permite sustituir un modelo de probabilidad por
otro. Aunque en teoría podemos determinar la ~calidad de la aproximación~ --por
medio de la ~expansión de Taylor~-- en la práctica puede resultar infactible dar
una estimación de este error.

#+REVEAL: split
El problema de la aproximación de Laplace es que utiliza información local y puede fallar en
capturar propiedades globales importantes de la distribución objetivo.

#+DOWNLOADED: screenshot @ 2022-05-09 10:39:33
#+caption: Imagen tomada de citep:Bishop2006. Aproximación de Laplace en rojo. Distribución objetivo sombreada. 
#+attr_html: :width 700 :align center
#+attr_latex: :width 0.65\textwidth
[[file:images/20220509-103933_screenshot.png]]


** Aproximación por optimización

Una alternativa es definir una familia de posibles distribuciones $\mathcal{Q}$
y encontrar dentro de esta familia de distribuciones la que ~mejor se parezca~ a
nuestra distribución objetivo $\pi(\theta | y)$.

#+REVEAL: split
Lo importante es poder definir la noción de encontrar al mejor candidato dentro de $\mathcal{Q}$. 

** La solución

En inferencia aproximada buscamos sustituir nuestra distribución objetivo con
aquella que resuelva el problema
\begin{align}
\min_{q \in \mathcal{Q}} \mathsf{KL}\bigg( q(\theta) \bigg \| \pi(\theta| y)\bigg)\,,
\end{align}
donde la familia de distribuciones $\mathcal{Q}$ define la calidad/complejidad
de nuestra aproximación.

#+REVEAL: split
El problema es que no podemos calcular la divergencia de KL, pues necesitamos
calcular la constante de normalización en $\pi(\theta|y)$. Así que lo que hacemos es re-expresar
\begin{align}
\mathsf{KL}( q(\theta) \| \pi(\theta| y))  = \mathbb{E}[\log q(\theta)] - \mathbb{E}[\log \pi(\theta, y)] + \log \pi(y)\,.
\end{align}

#+REVEAL: split
El lado derecho en el primer término define
\begin{align}
\mathsf{ELBO}(q) := \mathbb{E}[\log \pi(\theta, y)] - \mathbb{E}[\log q(\theta)]\,.
\end{align}
la cual se denomina la cota inferior de evidencia (/evidence lower bound/, ~ELBO~).

#+REVEAL: split
La cual podemos usar para re-expresar
\begin{align}
\log \pi(y) = \mathsf{KL}( q(\theta) \| \pi(\theta| y))   + \mathsf{ELBO}(q) \,,
\end{align}
de donde podemos ver que lo que podemos buscar es ~maximizar~ el ~ELBO~ en lugar de
~minimizar~ la divergencia ~KL~.

#+REVEAL: split
Nota que también podemos expresar
\begin{align}
\mathsf{ELBO}(q) = \mathbb{E}[\log \pi(y | \theta)] - \mathsf{KL}(q(\theta) \| \pi(\theta))\,.
\end{align}
Lo cual nos dice que la distribución $q \in \mathcal{Q}$ que encontraremos será
aquella que busque configuraciones afines al proceso generador de datos y que
sea cercana a la distribución inicial.

#+REVEAL: split
En [[fig:vi-logistic]] se muestra la solución encontrada minimizando el criterio de ~ELBO~. 

#+DOWNLOADED: screenshot @ 2022-05-09 10:43:36
#+name: fig:vi-logistic
#+caption: Imagen tomada de citep:Bishop2006. Solución de ~ELBO~ se muestra en verde. Aproximación de Laplace en rojo.
#+attr_html: :width 700 :align center
#+Attr_Latex: :width .65\linewidth
[[file:images/20220509-104336_screenshot.png]]

** Dirección de ~KL~

Hemos tomado la solución de $\mathsf{KL}(q\|\pi)$ por cuestiones numéricas y
también discutimos que la solución tiene la interpretación de ser una
aproximación de la posterior (justo lo que nos interesa).

#+REVEAL: split
Por ejemplo, en [[fig:vi-reversed]], bajo una familia de Gaussianas independientes
para $\mathcal{Q}$ la solución de $\mathsf{KL}(q\|\pi)$, además, se puede ver
como una distribución que se concentra en las zonas de ~alta
probabilidad~. Mientras que la solución de $\mathsf{KL}(\pi\|q)$ se concentra en
zonas de ~alta densidad~. Lo que nos habla que la formulación correcta se fijará
en las propiedades que nos interesen.

#+DOWNLOADED: screenshot @ 2022-05-09 10:54:36
#+name: fig:vi-reversed
#+caption: Imagen tomada de citep:Bishop2006. En (a) se muestra $\mathsf{KL}(q\|\pi)$ y en (b) se muestra $\mathsf{KL}(\pi\|q)$ donde $q\in \mathcal{Q}$ y $\pi$ es la distribución objetivo. 
#+attr_html: :width 900 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20220509-105436_screenshot.png]]

#+REVEAL: split
El mismo efecto se muestra en [[fig:vi-mixture]] donde dependiendo de la formulación
se pueden recuperar ciertas propiedades de la distribución objetivo.

#+DOWNLOADED: screenshot @ 2022-05-09 12:19:44
#+name: fig:vi-mixture
#+caption: Imagen tomada de citep:Bishop2006. Modelo objetivo basado en una mezcla de Gaussianas (azul). En (a) se muestra la aproximación que minimiza $\mathsf{KL}(\pi\|q)$. En (b) y (c) se muestran mínimos globales que corresponden a $\mathsf{KL}(q\|\pi)$. 
#+attr_html: :width 900 :align center
[[file:images/20220509-121944_screenshot.png]]


** Conclusiones

La familia de distribuciones $\mathcal{Q}$ define la calidad de
aproximación. Por simplicidad se utiliza una distribución Gaussiana con
componentes independientes en el espacio de parámetros transformados (~solución
de campo medio~, /mean field/).

#+DOWNLOADED: screenshot @ 2022-05-09 12:33:06
#+caption: Imagen tomada de citep:Kucukelbir2015.
#+attr_html: :width 1200 :align center
[[file:images/20220509-123306_screenshot.png]]

#+REVEAL: split
Dado que la solución de
\begin{align}
\min_{q \in \mathcal{Q}} \mathsf{KL}(q \| \pi)\,,
\end{align}
necesita resolverse en un espacio de funciones de probabilidad se utilizan
herramientas de ~cálculo de variaciones~. Por lo tanto, utilizar estas soluciones
para resolver un problema de inferencia se llama ~inferencia variacional~ o ~bayes
variacional~ (citep:Bishop2006). 

#+REVEAL: split
Sin embargo, es usual considerar ~familias parametrizadas~ y buscar 
\begin{align}
\min_{\omega \in \Omega} \mathsf{KL} \bigg(q_\omega(\theta) \bigg\| \pi (\theta | y) \bigg)\,,
\end{align}
donde la búsqueda se realiza mediante $\omega \in \Omega$. Por ejemplo, el
vector de medias y matriz de varianzas-covarianzas de las distribuciones
Gaussianas.

* Inferencia aproximada

~Stan~ en particular ofrece una solución basada en citep:Kucukelbir2015. En el cual se formula el problema en términos de:
- Diferenciación automática.
- Una familia $\mathcal{Q}$ de distribuciones que operan bajo un espacio sin
  restricciones.
- La familia $\mathcal{Q}$ es la familia de distribuciones Gaussianas con
  componentes independientes (la matriz de varianzas es una matriz diagonal).
- Se puede utilizar un modelo con matriz de varianzas completas:  ~method = "fullrank"~. 

* Ejemplo numérico

Tomado de la documentación de ~Stan~

#+begin_src stan :tangle ../modelos/variacional/bernoulli.stan
  data {
    int<lower=0> N;
    array[N] int<lower=0,upper=1> y;
  }

  parameters {
    real<lower=0, upper=1> theta;
  }

  model {
    theta ~ beta(1, 1);
    y ~ bernoulli(theta); 
  }
#+end_src

#+begin_src R :exports none :results none
  modelos_files <- "modelos/compilados/variacional"
  ruta <- file.path("modelos/variacional/bernoulli.stan")
  modelo <- cmdstan_model(ruta, dir = modelos_files)
#+end_src

#+begin_src R :exports code :results none
  stan_data <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))
  posterior <- modelo$sample(stan_data, seed = 123, chains = 2, refresh = 1000)
#+end_src

#+REVEAL: split
#+begin_src R :exports both :results org
  posterior$summary() |> as.data.frame()
#+end_src

#+RESULTS:
#+begin_src org
  variable  mean median   sd  mad     q5   q95 rhat ess_bulk ess_tail
1     lp__ -7.30  -7.03 0.72 0.38 -8.820 -6.75    1      902     1006
2    theta  0.25   0.23 0.12 0.13  0.079  0.47    1      762      712
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/variational-mcmc.jpeg :exports results :results output graphics file
  bayesplot::mcmc_hist(posterior$draws("theta")) + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/variational-mcmc.jpeg]]

#+REVEAL: split
#+begin_src R :exports both :results org
  posterior.advi <- modelo$variational(stan_data, seed = 123,
                                       output_samples = 2000)
#+end_src

#+RESULTS:
#+begin_src org
------------------------------------------------------------ 
EXPERIMENTAL ALGORITHM: 
  This procedure has not been thoroughly tested and may be unstable 
  or buggy. The interface is subject to change. 
------------------------------------------------------------ 
Gradient evaluation took 9e-06 seconds 
1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. 
Adjust your expectations accordingly! 
Begin eta adaptation. 
Iteration:   1 / 250 [  0%]  (Adaptation) 
Iteration:  50 / 250 [ 20%]  (Adaptation) 
Iteration: 100 / 250 [ 40%]  (Adaptation) 
Iteration: 150 / 250 [ 60%]  (Adaptation) 
Iteration: 200 / 250 [ 80%]  (Adaptation) 
Success! Found best value [eta = 1] earlier than expected. 
Begin stochastic gradient ascent. 
  iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes  
   100           -6.262             1.000            1.000 
   200           -6.263             0.500            1.000 
   300           -6.307             0.336            0.007   MEDIAN ELBO CONVERGED 
Drawing a sample of size 2000 from the approximate posterior...  
COMPLETED. 
Finished in  0.1 seconds.
#+end_src

#+REVEAL: split
#+begin_src R :exports both :results org 
  posterior.advi$summary() |>
  as.data.frame()
#+end_src

#+RESULTS:
#+begin_src org
     variable  mean median   sd  mad    q5     q95
1        lp__ -7.18  -6.94 0.59 0.26 -8.36 -6.7505
2 lp_approx__ -0.51  -0.22 0.69 0.30 -2.06 -0.0026
3       theta  0.26   0.25 0.12 0.11  0.11  0.4814
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/variacional-bernoulli-advi.jpeg :exports results :results output graphics file
  bayesplot::mcmc_hist(posterior.advi$draws("theta")) + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/variacional-bernoulli-advi.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/variacional-bernoulli-comp.jpeg :exports results :results output graphics file
  posterior$draws("theta", format = "df") |>
    rbind(posterior.advi$draws("theta", format = "df")) |>
    as_tibble() |>
    mutate(método = rep(c("mcmc", "advi"), each = 2000)) |>
    ggplot(aes(theta, group = método, fill = método)) +
    geom_histogram(position = "identity", alpha = .6) +
    sin_lineas
#+end_src

#+RESULTS:
[[file:../images/variacional-bernoulli-comp.jpeg]]

* Solución de campo medio

El supuesto de factorización
\begin{align}
q(\theta) = \prod_{j} q_j(\theta_j)\,,
\end{align}
es una estrategia bastante útil en física (/mean field theory/) y modelación de
sistemas expertos (/message passing/, citep:Koller2009).

#+REVEAL: split
La ventaja de esta factorización nos permite encontrar soluciones de forma
cerrada donde podemos considerar para cada $j$
\begin{align}
\log q_j^*(\theta_j) = \mathbb{E}_{q, i \neq j} \left( \log \pi(y, \theta) \right) + \mathsf{const}\,,
\end{align}
donde usualmente la solución tiene un expresión analítica para miembros de la
~familia exponencial~.

#+REVEAL: split
En la práctica, se desarrolla el modelo y se desarrollan las ecuaciones para resolver el problema de optimización variacional.

#+begin_quote
¿Cuánto tiempo tardas en escribir y resolver el problema variacional para una
aplicación? El mismo tiempo que tarda en converger tu MCMC. ---Frase popular en:
[[http://approximateinference.org/][Conferencia en inferencia aproximada]].
#+end_quote

* Conclusiones

- Inferencia variacional tiene poco de incorporarse a lenguajes de programación probabilística.
- Es una alternativa viable para poner en producción modelos bayesianos (por ejemplo, la sesión de [[https://www.youtube.com/watch?v=lxIJca62ezU&t=1230s][conferencia]] de [[https://www.smartly.io/][Smartly.io]]). 
- Tema activo de investigación que logra conjuntar el estado del arte en ML con formulación probabilista. 

#+REVEAL: split
#+begin_quote
Variational inference is an extremely flexible and powerful approximation method. Its downside is that constructing the bound and update equations can be tedious. For a quick test, variational inference is often not a good idea. But for a deployed product, ita can be the most powerful tool in the box. ---Philip Hennig, Probabilistic ML course, Tübingen U. 
#+end_quote

# * Referencias                                                         :latex:

bibliographystyle:abbrvnat
bibliography:references.bib

* COMMENT Notas

La sección 4.1.4 de citep:Murphy2012. 


