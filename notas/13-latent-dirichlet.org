#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Asignación Latente Dirichlet~
#+STARTUP: showall
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+EXCLUDE_TAGS: toc latex
#+PROPERTY: header-args:R :session latent-dirichlet :exports both :results output org :tangle ../rscripts/13-latent-dirichlet.R :mkdirp yes :dir ../

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

#+begin_src R :exports none :results none
  ## Librerias para modelacion bayesiana
  library(cmdstanr)
  library(posterior)
  library(bayesplot)
#+end_src

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Asignación Latente Dirichlet.\\
*Objetivo*: En esta sección veremos un modelo que entra en la frontera de inferencia Bayesiana y Aprendizaje de Máquina: modelo de asignación latente. El cual puede ser entendido como un método de segmentación probabilística. Las aplicaciones de este modelo son variadas y en la clase lo discutiremos en términos de procesamiento de documentos escritos. /Disclaimer/: Parte del material fue tomado del curso en métodos Bayesianos para Bioestadística impartido por Jeff Miller en Harvard en la escuela de Salud Pública, materiales [[https://jwmi.github.io/BMB/][aquí]].\\
*Lectura recomendada*: El modelo de asignación latente Dirichlet lo puedes encontrar en citep:Murphy2012 (pronto saldrá la nueva edición). Los artículos originales también son una buena referencia, citep:Blei2003. 
#+END_NOTES



* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#el-modelo-de-lda][El modelo de LDA]]
  - [[#modelo-generativo][Modelo generativo]]
    - [[#definición-distribución-dirichlet][Definición (Distribución Dirichlet):]]
  - [[#observaciones-del-modelo][Observaciones del modelo]]
  - [[#modelo-variacional][Modelo variacional]]
  - [[#observaciones-del-método-variacional][Observaciones del método variacional]]
  - [[#aplicación-associated-press][Aplicación: Associated Press]]
- [[#extensiones-del-modelo][Extensiones del modelo]]
- [[#mas-extensiones][Mas extensiones]]
- [[#set-de-herramientas][Set de herramientas]]
- [[#referencias][Referencias]]
:END:


* Introducción

- VI es una estrategia para poder hacer ~inferencia Bayesiana por medio de aproximaciones~ a la distribución posterior.
- La idea es:
  1. Escoger una familia de distribuciones $\mathcal{Q}$ .
  2. Encontrar el elemento $q \in \mathcal{Q}$ mas cercano a la distribución posterior.
  3. Utilizar $q^*$ para resolver los problemas de inferencia.

#+REVEAL: split
- Aplicación de inferencia variacional en problemas /machine learning/ bajo una formulación probabilista.
- Modelo para una ~colección de documentos~.
- Cada documento es una colección de ~palabras~ donde cada palabra se extrae de un
  ~tema~ en particular.
- Los temas definen las palabras que se utilizarán para escribir el documento.
- Los documentos pueden tener mas de un tema.
- Las palabras son intercambiables.


#+REVEAL: split
- Con ~asignación latente Dirichlet~ (LDA), modelamos los tópicos de una colección de observaciones.
- Usualmente se utiliza para datos no-estructurados:
  - Imágenes, datos genómicos o redes sociales.
- Se puede utilizar para datos en /stream/.
- Catalogación automática de objetos.

#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-16 17:26:52
#+caption: Imagen tomada de citep:Blei2012.
#+attr_html: :width 1200 :align center
[[file:images/20220516-172652_screenshot.png]]

#+REVEAL: split

#+DOWNLOADED: screenshot @ 2022-05-16 17:28:49
#+caption: Imagen tomada de citep:Blei2012.
#+attr_html: :width 1200 :align center
[[file:images/20220516-172849_screenshot.png]]


* El modelo de LDA

Asumimos que cada documento puede hablar de distintos temas al mismo tiempo (por ejemplo, discursos). 

#+DOWNLOADED: screenshot @ 2022-05-16 22:07:55
#+caption: Modelo de tópicos sobre documentos en términos de contenido. Imagen tomada del curso de /Probabilistic ML/ de Phillip Hennig. 
#+attr_html: :width 1200 :align center
[[file:images/20220516-220755_screenshot.png]]

#+REVEAL: split

Buscamos una representación con pocas entradas activas pues cada documento no
podría hablar de todos los temas de conversación.

#+caption: Diferencia entre modelo de mezcla y modelode tópicios sobre documentos en términos de contenido. Imagen tomada del curso de /Probabilistic ML/ de Phillip Hennig. 
#+attr_html: :width 1200 :align center
[[file:images/20220516-221141_screenshot.png]]

#+REVEAL: split
En particular usaremos distribuciones Dirichlet pues nos permiten tener una
representación ~rala~ (/sparse/) de los componentes que estarán activos en nuestras
observaciones.

#+DOWNLOADED: screenshot @ 2022-05-16 22:15:35
#+caption: Diferencia entre modelos de tópicos usando una distribución ~densa~ y una distribución ~rala~. Imagen tomada del curso de /Probabilistic ML/ de Phillip Hennig. 
#+attr_html: :width 1200 :align center
[[file:images/20220516-221535_screenshot.png]]



#+REVEAL: split
- Supongamos que existen $K$ temas, $n$ documentos, $L_i$ palabras en el
  documento $i$, y $V$ palabras en el vocabulario.
- Cada documento tiene:
  - $w_{ik}$: la proporción del documento que proviene del tema $k$.
  - $z_{i\ell}$: el tema de la palabra $\ell$.
  - $x_{i\ell}$: la palabra en la posición $\ell$.
- De manera global definimos $\beta_{kv}$: la frecuencia con la que aparece la palabra $v$ en el tema $k$.

** Modelo generativo

Consideremos un modelo donde las palabras tienen una asignación de tema y, además, cada palabra es una realización aleatoria de acuerdo al tópico y a la colección de posibles palabras que se utilizan en dicho tema
\begin{gather}
Z_{i\ell} | w  \sim \mathsf{Categorical}(w_i)\,,\\
x_{i\ell}  | \beta, Z_{i\ell} = k \sim \mathsf{Categorical}(\beta_k)\,,
\end{gather}
de manera independiente para cada $i \in \{1, \ldots, n\}$ y $\ell \in \{1, \ldots, L_i\}$.

#+REVEAL: split
Nota que
\begin{align}
w_i = (w_{i1}, \ldots, w_{iK})^\top, \qquad \beta_k = (\beta_{k1}, \ldots, \beta_{kV})^\top\,.
\end{align}

La distribución previa es
\begin{gather}
w_i \sim \mathsf{Dirichlet}(\alpha_1, \ldots, \alpha_K)\,,\\
\beta_k \sim \mathsf{Dirichlet}(\lambda_1, \ldots, \lambda_V)\,.
\end{gather}

*** Definición (*Distribución Dirichlet*):
:PROPERTIES:
:reveal_background: #00468b
:END:
Decimos que un vector aleatorio $w\in \mathbb{R}^K$ tiene una distribución $\mathsf{Dirichlet}(\alpha)$ con $\alpha \in \mathbb{R}^K_+$ si su función de densidad es
\begin{align}
\pi(w | \alpha ) = \frac{\Gamma \left( \sum_{k = 1}^{K} \alpha_k \right)}{\prod_{k}^{} \Gamma(\alpha_k)} \cdot \prod_{k}^{} w_k^{\alpha_k - 1}\,, \qquad \sum_k w_k = 1\,.
\end{align}



#+BEGIN_NOTES
La ~distribución Dirichlet~ es una generalización para la ~distribución Beta~. Es
usual utilizar una distribución Dirichlet para el vector de probabilidades de un
~modelo multinomial~. En este sentido la distribución inicial Dirichlet y
el modelo Multinomial forman un ~modelo conjugado~.

Aplicaciones clásicas de éstos modelos también se pueden encontrar en ~modelos de
mezclas~ donde los pesos en la mezcla se consideran realizaciones aleatorias de
un distribución Dirichlet.


Además, nota que el vector aleatorio es un vector de longitud fija. Si
quisiéramos modelar un vector donde el número de entradas es aleatoria entonces
podemos considerar un ~proceso Dirichlet~.
#+END_NOTES

#+REVEAL: split

El modelo completo queda escrito como en [[fig:lda-model]] donde queda claro que la
estructura condicional del modelo es bastante compleja pero que es relativamente sencillo
resolver utilizando muestreo de Gibbs. 

#+DOWNLOADED: screenshot @ 2022-05-16 22:30:23
#+name: fig:lda-model
#+caption: Modelo completo en asignación de temas. Imagen tomada del curso de /Probabilistic ML/ de Phillip Hennig.
#+attr_html: :width 1200 :align center
[[file:images/20220516-223023_screenshot.png]]


** Observaciones del modelo

- El orden no afecta la composición del modelo (/bag of words/). 
- No es un buen modelo de lenguaje, pero ayuda a generar conocimiento de los documentos.
- El modelo es invariante al orden en el que estudiamos los documentos. 

** Modelo variacional

- La distribución objetivo es la posterior $\pi(z, w, \beta | x)$.
- Se consideran modelos
  \begin{align}
  q(z, w, \beta) = q(z) \, q(w) \, q(\beta)\,.
  \end{align}
#+REVEAL: split
- El modelo variacional obtiene
  \begin{gather}
  q(w) = \prod_{i = 1}^{n} \mathsf{Dirichlet}(w_i | r_{i1}, \ldots, r_{iK})\,,\\
  q(\beta) = \prod_{k = 1}^{K} \mathsf{Dirichlet}(\beta_k | s_{k1}, \ldots, s_{kV})\,,\\
  q(z) = \prod_{i = 1}^{n} \prod_{\ell = 1}^{L_i} \mathsf{Categorical}(z_{i\ell} | t_{i\ell})\,,
  \end{gather}
  en donde cada término explota la estructura conjugada del modelo. 

** Observaciones del método variacional

- Nota que aunque hemos asumido una factorización del estilo $q(z, w, \beta) = q(z) \, q(w) \, q(\beta)$  el modelo en si obtiene
  \begin{align}
  q(z, w, \beta) = \left( \prod_{i,\ell} q(z_{i\ell}) \right) \, \left( \prod_{i} q(w_i) \right) \, \left( \prod_k q(\beta_k) \right)\,.
  \end{align}
- La funciones de densidad óptimas (en ~KL~) son distribuciones ~Dirichlet~. 


** Aplicación: Associated Press

- Ejemplo original en citep:Blei2003.
- Contiene $n = 16,333$ artículos.
- Contiene $V = 23,075$ palabras.
- Se necesitan eliminar palabras sin contenido informativo (/stop-words/).
- Se define un número de tópicos $K= 100$.
- El artículo original solo usa ~VI~ en $z, w$.

#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-16 18:37:38
#+caption: Resultados de citep:Blei2003.
#+attr_html: :width 700 :align center
[[file:images/20220516-183738_screenshot.png]]


* Extensiones del modelo

- LDA y un ~modelo de estados ocultos~: captura de dependencias en palabras cercanas.
- Modelo no-paramétrico basado en un ~proceso Dirichlet~.
- Modelo dinámico: cómo cambian los tópicos a lo largo del tiempo.
- Modelo jerárquico de tópicos (temas): de lo mas general a lo mas particular.
- Extensiones con meta-datos: autor, títulos de documentos, afiliaciones, etc.


#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-16 18:41:35
#+caption: Imagen tomada de citep:Blei2012. 
#+attr_html: :width 700 :align center
[[file:images/20220516-184135_screenshot.png]]


* Mas extensiones

- LDA con temas correlacionados, citet:Blei2007. 
- LDA en línea, citet:Hoffman2010.
- LDA en paralelo, citet:Zhai2012.
- LDA multilenguajes, citet:Hu2014. 
- Inferencia automática (~Infer.NET~).   

* /Set/ de herramientas

En el curso  hemos aprendido:
\begin{align}
\int h(\theta) \, \pi(\theta) \,  \text{d}\theta, \qquad \pi(x, \theta) = \pi( x | \theta ) \pi(\theta)\,, \qquad \pi(\theta | y ) = \frac{\pi(y| \theta) \pi(\theta)}{\pi(y)}\,.
\end{align}

#+REVEAL: split
En términos de modelado:
- Modelos bayesianos.
- Modelos predictivos probabilísticos.
- Comparación de modelos.
- Crítica de modelos.

#+REVEAL: split
En términos computacionales:
- Monte Carlo.
- Monte Carlo vía Cadenas de Markov.
- Inferencia variacional. 


* Referencias                                                         :latex:

bibliographystyle:abbrvnat
bibliography:references.bib
